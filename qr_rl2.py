# -*- coding: utf-8 -*-
"""QR_RL2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jtPlRdgz_jk3Qu8Sb255kbex3UavJHPb
"""

# CELL 0 - (Optional) Installs - run this cell first in Colab
!pip install -q pennylane torch gym gym-minigrid==1.0.0 tqdm

# ===== CELL 1 - Imports & seed =====
import os
import math
import time
import random
from collections import deque

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from tqdm.auto import tqdm

import pennylane as qml

# reproducibility
SEED = 123
random.seed(SEED)
np.random.seed(SEED)
torch.manual_seed(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# ===== CELL 2 - SharedAdam (from A3C-style repos) =====

class SharedAdam(optim.Adam):
    """
    SharedAdam: compatible with modern PyTorch's Adam API.
    Initializes state with singleton tensors for 'step' to avoid the
    "state_steps must contain a list of singleton tensors" RuntimeError.
    Works in single-process runs and is ready for multiprocessing (share_memory_()).
    """
    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8,
                 weight_decay=0, amsgrad=False):
        super().__init__(params, lr=lr, betas=betas, eps=eps,
                         weight_decay=weight_decay, amsgrad=amsgrad)

        # Initialize state entries for each parameter so the optimizer is ready to share memory.
        for group in self.param_groups:
            for p in group['params']:
                if p is None:
                    continue
                state = self.state[p]
                # `step` must be a singleton tensor (not Python int) for the new API.
                if 'step' not in state or not torch.is_tensor(state['step']):
                    state['step'] = torch.zeros(1, dtype=torch.int64)
                if 'exp_avg' not in state:
                    state['exp_avg'] = torch.zeros_like(p.data)
                if 'exp_avg_sq' not in state:
                    state['exp_avg_sq'] = torch.zeros_like(p.data)
                # If you plan to use multiprocessing, uncomment these share_memory_() calls.
                try:
                    state['exp_avg'].share_memory_()
                    state['exp_avg_sq'].share_memory_()
                    state['step'].share_memory_()
                except Exception:
                    # share_memory_ may fail on some backends; ignore in single-process runs
                    pass

# ===== CELL 3 - Frozen VQC (quantum reservoir) =====
# This implements a parameterized circuit whose parameters are randomly
# initialized and then frozen. The circuit maps a small real vector input
# (angle encoding) to a scalar (mean Z expectation).
from functools import reduce
from operator import add


class FrozenVQC:
    """
    Frozen variational circuit acting as a quantum reservoir.
    Ensures outputs are returned as torch.float32 on the configured device.
    """
    def __init__(self, n_qubits=3, n_layers=1, seed=None, device_name="default.qubit"):
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.dev = qml.device(device_name, wires=self.n_qubits)

        rng = np.random.RandomState(seed if seed is not None else np.random.randint(0, 99999))
        # Use float64 numpy values but store as float32 torch to avoid most dtype issues
        params_np = rng.normal(scale=0.5, size=(n_layers, n_qubits, 3)).astype(np.float64)
        self._params_torch = torch.tensor(params_np, dtype=torch.float32, device=device, requires_grad=False)

        # mean-Z Hamiltonian
        obs = [qml.PauliZ(i) for i in range(self.n_qubits)]
        coeffs = [1.0 / float(self.n_qubits)] * self.n_qubits
        self._ham = qml.Hamiltonian(coeffs, obs)

        @qml.qnode(self.dev, interface='torch')
        def circuit(inputs, params):
            # angle-encode inputs with RX
            for i in range(self.n_qubits):
                angle = inputs[i] if i < len(inputs) else 0.0
                qml.RX(angle, wires=i)

            # frozen variational layers
            for l in range(self.n_layers):
                for q in range(self.n_qubits):
                    a = params[l, q, 0]
                    b = params[l, q, 1]
                    c = params[l, q, 2]
                    qml.RX(a, wires=q)
                    qml.RY(b, wires=q)
                    qml.RZ(c, wires=q)
                for q in range(self.n_qubits - 1):
                    qml.CNOT(wires=[q, q+1])

            # return expectation of mean-Z Hamiltonian
            return qml.expval(self._ham)

        self._circuit = circuit

    def __call__(self, x):
        """
        x: 1D numpy array or 1D torch tensor (length <= n_qubits)
        returns: torch tensor shape (1,) dtype=torch.float32 on 'device'
        """
        # convert input to float32 torch tensor on device
        if isinstance(x, np.ndarray):
            xt = torch.tensor(x, dtype=torch.float32, device=device)
        elif isinstance(x, torch.Tensor):
            xt = x.to(device).float()
        else:
            xt = torch.tensor(np.array(x), dtype=torch.float32, device=device)

        # pad/truncate to n_qubits
        if xt.numel() < self.n_qubits:
            pad = torch.zeros(self.n_qubits - xt.numel(), dtype=torch.float32, device=device)
            xt = torch.cat([xt, pad], dim=0)
        elif xt.numel() > self.n_qubits:
            xt = xt[:self.n_qubits]

        out = self._circuit(xt, self._params_torch)

        # qnode may return float64; force float32 and correct device
        if not torch.is_tensor(out):
            out = torch.tensor(out, dtype=torch.float32, device=device)
        else:
            out = out.to(device).float()

        return torch.reshape(out, (1,))

# ===== CELL 4 - QLSTM cell (gates computed by frozen VQCs) =====
class QLSTMCell(nn.Module):
    """
    QLSTMCell: Uses FrozenVQC circuits to compute gate scalars. These scalars are
    then projected to hidden dimension with small trainable linear layers.
    The frozen circuits act as a quantum reservoir (random/frozen parameters).
    """
    def __init__(self, input_size, hidden_size, n_qubits=3, n_layers=1, seed=None):
        super().__init__()
        self.input_size = input_size
        self.hidden_size = hidden_size

        # small classical preprocessing to match circuit input size
        self.pre_fc = nn.Linear(input_size + hidden_size, n_qubits)

        # four frozen circuits for i, f, o, g gates (reservoir)
        seed = seed if seed is not None else 0
        self.vqc_i = FrozenVQC(n_qubits=n_qubits, n_layers=n_layers, seed=seed+1)
        self.vqc_f = FrozenVQC(n_qubits=n_qubits, n_layers=n_layers, seed=seed+2)
        self.vqc_o = FrozenVQC(n_qubits=n_qubits, n_layers=n_layers, seed=seed+3)
        self.vqc_g = FrozenVQC(n_qubits=n_qubits, n_layers=n_layers, seed=seed+4)

        # projection layers from scalar outputs -> hidden vector
        self.i_proj = nn.Linear(1, hidden_size)
        self.f_proj = nn.Linear(1, hidden_size)
        self.o_proj = nn.Linear(1, hidden_size)
        self.g_proj = nn.Linear(1, hidden_size)

    def forward(self, x, hx):
        """
        x: [batch, input_size]
        hx: tuple of (h, c), each [batch, hidden_size]
        returns (h_new, c_new)
        """
        h, c = hx
        batch = x.shape[0]
        cat = torch.cat([x, h], dim=1)          # [batch, input+hidden]
        pre = torch.tanh(self.pre_fc(cat))      # [batch, n_qubits]

        h_list, c_list = [], []
        # process batch items (per-sample QNode calls)
        for b in range(batch):
            p = pre[b]  # length n_qubits
            i_s = self.vqc_i(p)   # scalar tensor
            f_s = self.vqc_f(p)
            o_s = self.vqc_o(p)
            g_s = self.vqc_g(p)

            # project scalars -> vectors
            i_v = torch.sigmoid(self.i_proj(i_s))
            f_v = torch.sigmoid(self.f_proj(f_s))
            o_v = torch.sigmoid(self.o_proj(o_s))
            g_v = torch.tanh(self.g_proj(g_s))

            c_prev = c[b]
            c_new = f_v * c_prev + i_v * g_v
            h_new = o_v * torch.tanh(c_new)
            h_list.append(h_new.unsqueeze(0))
            c_list.append(c_new.unsqueeze(0))

        h_new = torch.cat(h_list, dim=0)
        c_new = torch.cat(c_list, dim=0)
        return h_new, c_new

# ===== CELL 5 - QLSTM wrapper for sequences =====
class QLSTMReservoir(nn.Module):
    def __init__(self, input_size, hidden_size, n_qubits=3, n_layers=1, seed=None):
        super().__init__()
        self.cell = QLSTMCell(input_size, hidden_size, n_qubits=n_qubits, n_layers=n_layers, seed=seed)

    def forward(self, seq):
        """
        seq: [seq_len, batch, input_size]
        returns final h, c
        """
        seq_len, batch, _ = seq.shape
        h = torch.zeros(batch, self.cell.hidden_size, device=device)
        c = torch.zeros(batch, self.cell.hidden_size, device=device)
        for t in range(seq_len):
            h, c = self.cell(seq[t], (h, c))
        return h, c

# ===== CELL 6 - ActorCritic model using QLSTM reservoir =====
class ActorCriticReservoir(nn.Module):
    def __init__(self, obs_size, n_actions, qlstm_hidden=32, n_qubits=3, n_layers=1, seed=None):
        super().__init__()
        self.qlstm = QLSTMReservoir(obs_size, qlstm_hidden, n_qubits=n_qubits, n_layers=n_layers, seed=seed)
        self.policy = nn.Sequential(
            nn.Linear(qlstm_hidden, 64),
            nn.ReLU(),
            nn.Linear(64, n_actions)
        )
        self.value = nn.Sequential(
            nn.Linear(qlstm_hidden, 64),
            nn.ReLU(),
            nn.Linear(64, 1)
        )

    def forward(self, seq):
        """
        seq: [seq_len, batch, obs_size]
        returns: logits [batch, n_actions], value [batch]
        """
        h, _ = self.qlstm(seq)
        logits = self.policy(h)
        value = self.value(h).squeeze(-1)
        return logits, value

# ===== CELL 7 - Small custom partially-observable environment(s) (no gym) =====
class PartialGridEnv:
    """
    Simple 1D grid world (positions 0..L-1). Agent starts at pos 0.
    Goal is at pos L-1. Actions: 0=left, 1=right.
    Observation is partial: the agent only sees its local position modulo some noise,
    or a small window; this creates partial observability requiring memory.
    Each step: reward -0.01, +1 on reaching goal. Episode ends on goal or max_steps.
    """
    def __init__(self, length=5, max_steps=20, seed=None, noise=0.0):
        self.length = length
        self.max_steps = max_steps
        self.noise = noise
        self.rng = np.random.RandomState(seed if seed is not None else None)
        self.reset()

    def reset(self):
        self.pos = 0
        self.steps = 0
        # observation: 1D vector with single value = pos / (length-1) + noise
        obs = np.array([self.pos / max(1, self.length - 1)], dtype=np.float32)
        obs += self.noise * self.rng.randn(*obs.shape)
        self.done = False
        return obs

    def step(self, action):
        """
        action: int 0 or 1
        returns obs, reward, done, info
        """
        if self.done:
            return self.reset(), 0.0, True, {}

        # action interpretation
        if action == 1:
            self.pos = min(self.length - 1, self.pos + 1)
        else:
            self.pos = max(0, self.pos - 1)

        self.steps += 1
        reward = -0.01
        if self.pos == self.length - 1:
            reward += 1.0
            self.done = True

        if self.steps >= self.max_steps:
            self.done = True

        obs = np.array([self.pos / max(1, self.length - 1)], dtype=np.float32)
        obs += self.noise * self.rng.randn(*obs.shape)
        return obs, reward, self.done, {}

# ===== CELL 8 - Synchronous vector runner (no gym) and helpers =====
def make_env_fn(length=5, max_steps=20, seed=None, noise=0.0):
    def _thunk():
        return PartialGridEnv(length=length, max_steps=max_steps, seed=seed, noise=noise)
    return _thunk

def select_action_from_logits(logits):
    """
    logits: torch tensor [batch, n_actions]
    returns: actions (np array length batch), log_probs torch [batch], entropy torch [batch]
    """
    probs = torch.softmax(logits, dim=-1)
    dist = torch.distributions.Categorical(probs)
    actions = dist.sample()
    return actions.cpu().numpy(), dist.log_prob(actions), dist.entropy()

# ===== CELL 9 - Returns & advantages (simple n-step returns) =====
def compute_returns(rewards, masks, next_value, gamma=0.99):
    """
    rewards: list of t tensors [batch]
    masks: list of t tensors (1 if not done else 0)
    next_value: tensor [batch]
    returns: list of t tensors [batch] where R_t = sum_{k=0..} gamma^k r_{t+k} + gamma^n * next_value
    """
    R = next_value
    returns = []
    for step in reversed(range(len(rewards))):
        R = rewards[step] + gamma * R * masks[step]
        returns.insert(0, R)
    return returns

# ===== CELL 10 - Training loop (A3C-style synchronous multi-workers, uses tqdm) =====
def train_qlstm_a3c(num_workers=4, env_maker=None, obs_size=1, n_actions=2,
                    total_updates=800, t_max=5, gamma=0.99, lr=1e-3,
                    qlstm_hidden=32, n_qubits=3, n_layers=1, seed=SEED):
    # create environments
    envs = [env_maker() for _ in range(num_workers)]
    obs = np.stack([env.reset() for env in envs], axis=0)  # [num_workers, obs_dim]
    obs_t = torch.tensor(obs, dtype=torch.float32, device=device)

    model = ActorCriticReservoir(obs_size, n_actions, qlstm_hidden, n_qubits, n_layers, seed=seed).to(device)
    optimizer = SharedAdam(model.parameters(), lr=lr)

    episode_rewards = np.zeros(num_workers, dtype=np.float32)
    epinfo = deque(maxlen=100)

    pbar = tqdm(range(total_updates), desc="A3C updates")
    global_step = 0

    for update in pbar:
        # storage for t_max steps
        obs_seq = []
        actions_seq = []
        logp_seq = []
        rewards_seq = []
        masks_seq = []
        values_seq = []

        # initial hidden (we'll feed sequence length 1 at each step)
        for step in range(t_max):
            # prepare seq tensor shape [seq_len=1, batch, obs_size]
            seq = obs_t.unsqueeze(0)  # [1, batch, obs]
            logits, value = model(seq)  # value: [batch]
            actions_np, logp, entropy = select_action_from_logits(logits)

            # step each env
            next_obs_list = []
            rewards = []
            dones = []
            for i, env in enumerate(envs):
                a = int(actions_np[i])
                next_o, r, d, _ = env.step(a)
                next_obs_list.append(next_o)
                rewards.append(r)
                dones.append(d)
                episode_rewards[i] += r
                if d:
                    epinfo.append(episode_rewards[i])
                    episode_rewards[i] = 0.0

            next_obs = np.stack(next_obs_list, axis=0)
            next_obs_t = torch.tensor(next_obs, dtype=torch.float32, device=device)

            obs_seq.append(seq)                            # [1,batch,obs]
            actions_seq.append(torch.tensor(actions_np, device=device))
            logp_seq.append(logp)
            rewards_seq.append(torch.tensor(rewards, dtype=torch.float32, device=device))
            masks_seq.append(torch.tensor([0.0 if d else 1.0 for d in dones], dtype=torch.float32, device=device))
            values_seq.append(value.detach())

            obs_t = next_obs_t
            global_step += 1

        # compute bootstrap next value
        with torch.no_grad():
            seq = obs_t.unsqueeze(0)
            _, next_value = model(seq)
            next_value = next_value.detach()

        # compute returns
        returns = compute_returns(rewards_seq, masks_seq, next_value, gamma=gamma)

        # compute losses across steps
        optimizer.zero_grad()
        policy_loss = 0.0
        value_loss = 0.0
        entropy_loss = 0.0

        for t in range(len(rewards_seq)):
            seq = obs_seq[t]   # [1,batch,obs]
            logits, value = model(seq)
            probs = torch.softmax(logits, dim=-1)
            log_probs_all = torch.log_softmax(logits, dim=-1)
            actions_t = actions_seq[t].long()
            log_prob_taken = log_probs_all.gather(1, actions_t.unsqueeze(1)).squeeze(1)
            entropy_t = -(probs * log_probs_all).sum(-1).mean()
            R = returns[t]
            advantage = (R - value).detach()

            policy_loss = policy_loss - (log_prob_taken * advantage).mean()
            value_loss = value_loss + 0.5 * (R - value).pow(2).mean()
            entropy_loss = entropy_loss + entropy_t

        total_loss = policy_loss + 0.5 * value_loss - 0.01 * entropy_loss
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)
        optimizer.step()

        # Logging
        if len(epinfo) > 0:
            pbar.set_postfix({"mean_ep_reward": float(np.mean(epinfo)), "global_step": global_step})
        else:
            pbar.set_postfix({"global_step": global_step})

    print("Training finished.")
    return model

# ===== CELL 11 - Run a quick experiment (no gym) =====
# Use the PartialGridEnv (partially observable) to exercise QLSTM memory.
env_maker = make_env_fn(length=5, max_steps=20, seed=SEED, noise=0.05)
trained_model = train_qlstm_a3c(num_workers=4, env_maker=env_maker,
                                obs_size=1, n_actions=2,
                                total_updates=200, t_max=5, gamma=0.99, lr=5e-4,
                                qlstm_hidden=32, n_qubits=3, n_layers=1)

import sys, math, time
import numpy as np
import torch
from tqdm.auto import tqdm
import matplotlib.pyplot as plt

# --- Settings ---
N_EVAL = 100          # total episodes to run for statistics
N_PRINT = 5           # how many episodes to print step-by-step
MAX_STEPS = 200       # safety cap per episode
DETERMINISTIC = True  # greedy actions (True) or sampling (False)
SHOW_PROGRESS = True  # use tqdm

# --- Sanity checks ---
if 'trained_model' not in globals():
    raise RuntimeError("trained_model not found in the notebook namespace. Make sure you trained and named the model `trained_model`.")
if 'env_maker' not in globals():
    raise RuntimeError("env_maker not found in the notebook namespace. Provide the environment factory as `env_maker` (callable returning env instances).")

model = trained_model
device = globals().get('device', torch.device('cpu'))

model.eval()

def run_episode(model, env, max_steps=MAX_STEPS, deterministic=True, verbose=False):
    """
    Run single episode; returns total_reward and a list of step tuples (obs, action, reward, value, probs).
    """
    obs = env.reset()
    obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)  # [1, obs_dim]
    total_reward = 0.0
    steps = []

    for step in range(max_steps):
        seq = obs_t.unsqueeze(0)  # [seq_len=1, batch=1, obs_size]
        with torch.no_grad():
            logits, value = model(seq)           # logits: [batch, n_actions], value: [batch]
            probs = torch.softmax(logits, dim=-1)
        probs_cpu = probs.detach().cpu().numpy().squeeze()
        value_scalar = float(value.item())

        if deterministic:
            action = int(torch.argmax(probs, dim=-1).item())
        else:
            dist = torch.distributions.Categorical(probs)
            action = int(dist.sample().item())

        next_obs, reward, done, _ = env.step(action)
        total_reward += float(reward)

        if verbose:
            print(f"Step {step:03d} | obs={np.round(obs.squeeze(),4).tolist()} | value={value_scalar:.4f} | action={action} | probs={np.round(probs_cpu,4).tolist()} | reward={reward:.4f}")

        steps.append((obs, action, reward, value_scalar, probs_cpu))
        obs = next_obs
        obs_t = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)

        if done:
            if verbose:
                print(f"Episode finished at step {step+1}, total_reward={total_reward:.4f}")
            break

    return total_reward, steps

# --- Evaluation loop ---
rewards = []
print(f"Running {N_EVAL} evaluation episodes (printing first {N_PRINT}) ...")
iterator = range(N_EVAL)
if SHOW_PROGRESS:
    iterator = tqdm(iterator, desc="Eval episodes")

for i in iterator:
    env = env_maker()  # new env instance
    verbose = (i < N_PRINT)
    total_r, steps = run_episode(model, env, max_steps=MAX_STEPS, deterministic=DETERMINISTIC, verbose=verbose)
    rewards.append(total_r)
    # small separator after printed episode
    if verbose:
        print("-" * 60)

rewards = np.array(rewards, dtype=np.float32)

# --- Print summary stats ---
print("\nEvaluation summary:")
print(f"  Episodes: {len(rewards)}")
print(f"  Mean reward: {rewards.mean():.4f}")
print(f"  Std reward:  {rewards.std():.4f}")
print(f"  Min reward:  {rewards.min():.4f}")
print(f"  Max reward:  {rewards.max():.4f}")

# --- Plots: histogram + line of episode rewards ---
plt.figure(figsize=(12,4))

plt.subplot(1,2,1)
plt.hist(rewards, bins=min(30, max(5, len(rewards)//4)))
plt.title("Histogram of episode rewards")
plt.xlabel("Episode reward")
plt.ylabel("Count")
plt.grid(True)

plt.subplot(1,2,2)
plt.plot(rewards, marker='o', linestyle='-')
plt.xlabel("Episode index")
plt.ylabel("Total reward")
plt.title("Episode rewards")
plt.grid(True)

plt.tight_layout()
plt.show()

# optional: show some percentiles
pctiles = [10,25,50,75,90]
print("\nReward percentiles:")
for p in pctiles:
    print(f"  {p}th percentile: {np.percentile(rewards, p):.4f}")

# Compare deterministic (greedy) vs stochastic (sampled) policies
def evaluate_policy(model, env_maker, n_episodes=100, deterministic=True):
    model.eval()
    rewards = []
    for _ in range(n_episodes):
        env = env_maker()
        obs = env.reset()
        total = 0.0
        for _ in range(200):
            seq = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0).unsqueeze(0)
            with torch.no_grad():
                logits, _ = model(seq)
                probs = torch.softmax(logits, dim=-1)
            if deterministic:
                action = int(torch.argmax(probs, dim=-1).item())
            else:
                action = int(torch.distributions.Categorical(probs).sample().item())
            obs, r, done, _ = env.step(action)
            total += float(r)
            if done:
                break
        rewards.append(total)
    rewards = np.array(rewards)
    return rewards

det_rewards = evaluate_policy(trained_model, env_maker, n_episodes=100, deterministic=True)
stoch_rewards = evaluate_policy(trained_model, env_maker, n_episodes=100, deterministic=False)

print("Deterministic: mean {:.4f}, std {:.4f}, min {:.4f}, max {:.4f}".format(det_rewards.mean(), det_rewards.std(), det_rewards.min(), det_rewards.max()))
print("Stochastic :   mean {:.4f}, std {:.4f}, min {:.4f}, max {:.4f}".format(stoch_rewards.mean(), stoch_rewards.std(), stoch_rewards.min(), stoch_rewards.max()))