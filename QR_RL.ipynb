{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pennylane"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g7Z8hLZrKX9o",
        "outputId": "869f244f-8467-49f2-d8b5-a9c98ba50fda"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pennylane\n",
            "  Downloading pennylane-0.42.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.16.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from pennylane) (3.5)\n",
            "Collecting rustworkx>=0.14.0 (from pennylane)\n",
            "  Downloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: autograd in /usr/local/lib/python3.12/dist-packages (from pennylane) (1.8.0)\n",
            "Collecting appdirs (from pennylane)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting autoray<0.8,>=0.6.11 (from pennylane)\n",
            "  Downloading autoray-0.7.2-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.12/dist-packages (from pennylane) (5.5.2)\n",
            "Collecting pennylane-lightning>=0.42 (from pennylane)\n",
            "  Downloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.32.4)\n",
            "Requirement already satisfied: tomlkit in /usr/local/lib/python3.12/dist-packages (from pennylane) (0.13.3)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.12/dist-packages (from pennylane) (4.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from pennylane) (25.0)\n",
            "Collecting diastatic-malt (from pennylane)\n",
            "  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from pennylane) (2.0.2)\n",
            "Collecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.42->pennylane)\n",
            "  Downloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 kB\u001b[0m \u001b[31m757.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (1.6.3)\n",
            "Requirement already satisfied: gast in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (0.6.0)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.12/dist-packages (from diastatic-malt->pennylane) (3.1.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pennylane) (2025.8.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.12/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\n",
            "Downloading pennylane-0.42.3-py3-none-any.whl (4.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autoray-0.7.2-py3-none-any.whl (930 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.8/930.8 kB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pennylane_lightning-0.42.0-cp312-cp312-manylinux_2_28_x86_64.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rustworkx-0.17.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m77.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m16.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy_openblas32-0.3.30.0.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: appdirs, scipy-openblas32, rustworkx, autoray, diastatic-malt, pennylane-lightning, pennylane\n",
            "Successfully installed appdirs-1.4.4 autoray-0.7.2 diastatic-malt-2.15.2 pennylane-0.42.3 pennylane-lightning-0.42.0 rustworkx-0.17.1 scipy-openblas32-0.3.30.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "qlstm_reservoir_rl.py\n",
        "Self-contained RL with a QLSTM-as-Reservoir policy head on a shaped GridWorld.\n",
        "(Quantum-inspired)\n",
        "\"\"\"\n",
        "\n",
        "from typing import Tuple, List, Optional\n",
        "import random\n",
        "import time\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# ===============================\n",
        "# Utilities & global configuration\n",
        "# ===============================\n",
        "def set_seed(seed: int = 123):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# ===============================\n",
        "# Environment: shaped GridWorld\n",
        "# ===============================\n",
        "class SimpleGridEnv:\n",
        "    \"\"\"\n",
        "    Minimal deterministic GridWorld:\n",
        "      - Agent starts top-left (0,0)\n",
        "      - Goal at bottom-right (size-1, size-1)\n",
        "      - Actions: 0=up, 1=right, 2=down, 3=left\n",
        "      - Observation: 2 channels (agent one-hot, goal one-hot), flattened to float32 vector\n",
        "      - Reward shaping: potential-based (Ng et al., 1999) + step penalty\n",
        "        r' = r + β (γ Φ(s') - Φ(s)) + step_penalty\n",
        "        where Φ(s) = -ManhattanDistance(agent, goal)\n",
        "      - Episode ends when goal reached or max_steps exceeded\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        size: int = 5,\n",
        "        max_steps: int = 50,\n",
        "        seed: Optional[int] = None,\n",
        "        step_penalty: float = -0.01,\n",
        "        shaping_coef: float = 0.1,\n",
        "        gamma: float = 0.99,\n",
        "    ):\n",
        "        self.size = int(size)\n",
        "        self.max_steps = int(max_steps)\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.step_penalty = float(step_penalty)\n",
        "        self.shaping_coef = float(shaping_coef)\n",
        "        self.gamma = float(gamma)\n",
        "\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.goal_pos = [self.size - 1, self.size - 1]\n",
        "        self.steps = 0\n",
        "        self._last_phi = 0.0\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    @property\n",
        "    def observation_shape(self) -> tuple:\n",
        "        return (2 * self.size * self.size,)\n",
        "\n",
        "    @property\n",
        "    def action_space_n(self) -> int:\n",
        "        return 4\n",
        "\n",
        "    def _phi(self, pos: List[int]) -> float:\n",
        "        # Negative manhattan distance (larger is better because less negative)\n",
        "        return - (abs(pos[0] - self.goal_pos[0]) + abs(pos[1] - self.goal_pos[1]))\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        self.agent_pos = [0, 0]\n",
        "        self.goal_pos = [self.size - 1, self.size - 1]\n",
        "        self.steps = 0\n",
        "        self._last_phi = self._phi(self.agent_pos)\n",
        "        return self._render_obs()\n",
        "\n",
        "    def step(self, action: int):\n",
        "        # Store previous potential\n",
        "        prev_phi = self._last_phi\n",
        "\n",
        "        # Move deterministically within bounds\n",
        "        if action == 0:       # up\n",
        "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
        "        elif action == 1:     # right\n",
        "            self.agent_pos[1] = min(self.size - 1, self.agent_pos[1] + 1)\n",
        "        elif action == 2:     # down\n",
        "            self.agent_pos[0] = min(self.size - 1, self.agent_pos[0] + 1)\n",
        "        elif action == 3:     # left\n",
        "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action (expected 0..3)\")\n",
        "\n",
        "        self.steps += 1\n",
        "        reached_goal = (self.agent_pos == self.goal_pos)\n",
        "        done = reached_goal or (self.steps >= self.max_steps)\n",
        "\n",
        "        base_reward = 1.0 if reached_goal else 0.0\n",
        "        new_phi = self._phi(self.agent_pos)\n",
        "        shaping = self.shaping_coef * (self.gamma * new_phi - prev_phi)\n",
        "\n",
        "        # add step penalty unless just reached goal this step\n",
        "        shaped_reward = base_reward + shaping + (0.0 if reached_goal else self.step_penalty)\n",
        "\n",
        "        self._last_phi = new_phi\n",
        "        obs = self._render_obs()\n",
        "        return obs, float(shaped_reward), bool(done), {}\n",
        "\n",
        "    def _render_obs(self) -> np.ndarray:\n",
        "        agent_ch = np.zeros((self.size, self.size), dtype=np.float32)\n",
        "        goal_ch = np.zeros((self.size, self.size), dtype=np.float32)\n",
        "        agent_ch[self.agent_pos[0], self.agent_pos[1]] = 1.0\n",
        "        goal_ch[self.goal_pos[0], self.goal_pos[1]] = 1.0\n",
        "        obs = np.concatenate([agent_ch.ravel(), goal_ch.ravel()]).astype(np.float32)\n",
        "        return obs\n",
        "\n",
        "# ===============================\n",
        "# QLSTM (classical, quantum-inspired)\n",
        "# ===============================\n",
        "class QLSTMCell(nn.Module):\n",
        "    \"\"\"\n",
        "    Classical LSTM-like cell with orthonormal-initialized gating to mimic 'unitary-like' stability.\n",
        "    This serves as a practical stand-in for a QLSTM reservoir (fast, dependency-free).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int, bias: bool = True):\n",
        "        super().__init__()\n",
        "        self.input_size = int(input_size)\n",
        "        self.hidden_size = int(hidden_size)\n",
        "        self.linear = nn.Linear(self.input_size + self.hidden_size, 4 * self.hidden_size, bias=bias)\n",
        "        self._orthonormal_init(self.linear.weight)\n",
        "        if bias:\n",
        "            nn.init.constant_(self.linear.bias, 0.0)\n",
        "\n",
        "    @staticmethod\n",
        "    def _orthonormal_init(w: torch.Tensor, gain: float = 1.0):\n",
        "        try:\n",
        "            rows, cols = w.shape\n",
        "            a = torch.randn(rows, cols)\n",
        "            q, _ = torch.linalg.qr(a)  # QR-based orthonormal\n",
        "            # crop or pad columns to match shape\n",
        "            q = q[:, :cols] if q.shape[1] >= cols else torch.nn.functional.pad(q, (0, cols - q.shape[1]))\n",
        "            with torch.no_grad():\n",
        "                w.copy_(gain * q)\n",
        "        except Exception:\n",
        "            nn.init.xavier_uniform_(w)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, h: torch.Tensor, c: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # x: (B, input), h/c: (B, hidden)\n",
        "        combined = torch.cat([x, h], dim=-1)\n",
        "        gates = self.linear(combined)\n",
        "        i, f, o, g = gates.chunk(4, dim=-1)\n",
        "        i = torch.sigmoid(i)\n",
        "        f = torch.sigmoid(f)\n",
        "        o = torch.sigmoid(o)\n",
        "        g = torch.tanh(g)\n",
        "        c_next = f * c + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "        return h_next, c_next\n",
        "\n",
        "class QLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    Thin wrapper providing single-step and sequence APIs.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size: int, hidden_size: int):\n",
        "        super().__init__()\n",
        "        self.input_size = int(input_size)\n",
        "        self.hidden_size = int(hidden_size)\n",
        "        self.cell = QLSTMCell(self.input_size, self.hidden_size)\n",
        "\n",
        "    def init_hidden(self, batch_size: int = 1, device: Optional[torch.device] = None):\n",
        "        device = device or torch.device(\"cpu\")\n",
        "        h = torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "        c = torch.zeros(batch_size, self.hidden_size, device=device)\n",
        "        return h, c\n",
        "\n",
        "    def step(self, x: torch.Tensor, h: torch.Tensor, c: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        return self.cell(x, h, c)\n",
        "\n",
        "    def forward_sequence(\n",
        "        self, x_seq: torch.Tensor, h0: Optional[torch.Tensor] = None, c0: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n",
        "        # x_seq: (B, T, input)\n",
        "        if x_seq.dim() != 3:\n",
        "            raise ValueError(\"x_seq must be (batch, seq_len, input_size)\")\n",
        "        B, T, _ = x_seq.shape\n",
        "        device = x_seq.device\n",
        "        if h0 is None or c0 is None:\n",
        "            h, c = self.init_hidden(B, device=device)\n",
        "        else:\n",
        "            h, c = h0, c0\n",
        "        outputs = []\n",
        "        for t in range(T):\n",
        "            xt = x_seq[:, t, :]\n",
        "            h, c = self.cell(xt, h, c)\n",
        "            outputs.append(h)\n",
        "        outputs = torch.stack(outputs, dim=1)  # (B, T, H)\n",
        "        return outputs, (h, c)\n",
        "\n",
        "# ===============================\n",
        "# Reservoir wrapper (stateful)\n",
        "# ===============================\n",
        "class ReservoirQLSTM(nn.Module):\n",
        "    \"\"\"\n",
        "    QLSTM used as a reservoir. Optionally freeze parameters.\n",
        "    Provides a stateful step() API that keeps (h,c) across time and reset() to clear state.\n",
        "    \"\"\"\n",
        "    def __init__(self, qlstm: QLSTM, freeze: bool = True, device: str = \"cpu\"):\n",
        "        super().__init__()\n",
        "        self.qlstm = qlstm\n",
        "        self.device = torch.device(device)\n",
        "        if freeze:\n",
        "            for p in self.qlstm.parameters():\n",
        "                p.requires_grad = False\n",
        "        self.h: Optional[torch.Tensor] = None\n",
        "        self.c: Optional[torch.Tensor] = None\n",
        "\n",
        "    @property\n",
        "    def feature_dim(self) -> int:\n",
        "        return self.qlstm.hidden_size\n",
        "\n",
        "    def reset(self, batch_size: int = 1):\n",
        "        self.h, self.c = self.qlstm.init_hidden(batch_size=batch_size, device=self.device)\n",
        "\n",
        "    def step(self, x_1xD: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_1xD: (1, input_dim) float32 tensor on self.device\n",
        "        returns: (1, feature_dim) current hidden state (h_t)\n",
        "        \"\"\"\n",
        "        if self.h is None or self.c is None:\n",
        "            self.reset(batch_size=x_1xD.shape[0])\n",
        "        h_next, c_next = self.qlstm.step(x_1xD, self.h, self.c)\n",
        "        # Update internal state\n",
        "        self.h, self.c = h_next.detach(), c_next.detach()\n",
        "        return self.h\n",
        "\n",
        "# ===============================\n",
        "# Actor-Critic network (heads)\n",
        "# ===============================\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim: int, n_actions: int, hidden: int = 128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden)\n",
        "        self.pi = nn.Linear(hidden, n_actions)  # logits\n",
        "        self.v = nn.Linear(hidden, 1)\n",
        "\n",
        "        # Orthogonal initialization is a good default for A2C/A3C\n",
        "        nn.init.orthogonal_(self.fc1.weight, gain=math.sqrt(2))\n",
        "        nn.init.orthogonal_(self.pi.weight, gain=0.01)\n",
        "        nn.init.orthogonal_(self.v.weight, gain=1.0)\n",
        "        nn.init.constant_(self.fc1.bias, 0.0)\n",
        "        nn.init.constant_(self.pi.bias, 0.0)\n",
        "        nn.init.constant_(self.v.bias, 0.0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        logits = self.pi(x)             # (B, A)\n",
        "        value = self.v(x).squeeze(-1)   # (B,)\n",
        "        return logits, value\n",
        "\n",
        "# ===============================\n",
        "# On-policy trainer with GAE\n",
        "# ===============================\n",
        "class OnPolicyTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        env: SimpleGridEnv,\n",
        "        reservoir: ReservoirQLSTM,\n",
        "        policy: ActorCritic,\n",
        "        device: str = \"cpu\",\n",
        "        gamma: float = 0.99,\n",
        "        gae_lambda: float = 0.95,\n",
        "        lr: float = 1e-4,\n",
        "        entropy_coef: float = 0.02,     # will anneal down\n",
        "        value_coef: float = 0.5,\n",
        "        max_grad_norm: float = 0.5,\n",
        "        curriculum: bool = False,\n",
        "    ):\n",
        "        self.env = env\n",
        "        self.reservoir = reservoir\n",
        "        self.policy = policy\n",
        "        self.device = torch.device(device)\n",
        "        self.gamma = float(gamma)\n",
        "        self.gae_lambda = float(gae_lambda)\n",
        "        self.entropy_coef_init = float(entropy_coef)\n",
        "        self.value_coef = float(value_coef)\n",
        "        self.max_grad_norm = float(max_grad_norm)\n",
        "        self.curriculum = curriculum\n",
        "\n",
        "        self.reservoir.to(self.device)\n",
        "        self.policy.to(self.device)\n",
        "        self.optimizer = optim.Adam(filter(lambda p: p.requires_grad, self.policy.parameters()), lr=lr)\n",
        "\n",
        "        # tracking\n",
        "        self.ep_return_ma = 0.0  # moving average of episodic returns\n",
        "        self.ma_tau = 0.95\n",
        "\n",
        "    def train(\n",
        "        self,\n",
        "        epochs: int = 400,\n",
        "        rollout_len: int = 32,\n",
        "        print_every: int = 10,\n",
        "        normalize_advantage: bool = True,\n",
        "        entropy_anneal: bool = True,\n",
        "    ):\n",
        "        obs = self.env.reset()\n",
        "        self.reservoir.reset(batch_size=1)\n",
        "        episode_return = 0.0\n",
        "        episodes = 0\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            rewards: List[torch.Tensor] = []\n",
        "            values: List[torch.Tensor] = []\n",
        "            logps: List[torch.Tensor] = []\n",
        "            entropies: List[torch.Tensor] = []\n",
        "            masks: List[torch.Tensor] = []\n",
        "\n",
        "            # Collect a rollout\n",
        "            for _ in range(rollout_len):\n",
        "                obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)  # (1, D)\n",
        "                feat = self.reservoir.step(obs_t)  # (1, F)\n",
        "\n",
        "                logits, value = self.policy(feat)  # logits: (1, A), value: (1,)\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                dist = torch.distributions.Categorical(probs=probs)\n",
        "                action = dist.sample()\n",
        "                logp = dist.log_prob(action)              # (1,)\n",
        "                entropy = dist.entropy().mean()           # scalar\n",
        "\n",
        "                next_obs, reward, done, _ = self.env.step(int(action.item()))\n",
        "                episode_return += reward\n",
        "\n",
        "                rewards.append(torch.tensor(reward, dtype=torch.float32, device=self.device))\n",
        "                values.append(value.squeeze(0))           # (1,) -> ()\n",
        "                logps.append(logp.squeeze(0))             # (1,) -> ()\n",
        "                entropies.append(entropy)\n",
        "                masks.append(torch.tensor(0.0 if done else 1.0, dtype=torch.float32, device=self.device))\n",
        "\n",
        "                obs = next_obs\n",
        "\n",
        "                if done:\n",
        "                    # episode ends: reset env and reservoir state\n",
        "                    obs = self.env.reset()\n",
        "                    self.reservoir.reset(batch_size=1)\n",
        "                    episodes += 1\n",
        "                    # update moving average of returns\n",
        "                    self.ep_return_ma = self.ma_tau * self.ep_return_ma + (1 - self.ma_tau) * episode_return\n",
        "                    episode_return = 0.0\n",
        "\n",
        "            # Bootstrap next value from last obs\n",
        "            obs_t = torch.tensor(obs, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "            feat = self.reservoir.step(obs_t)  # note: keeps state continuity across rollouts\n",
        "            with torch.no_grad():\n",
        "                _, next_value = self.policy(feat)\n",
        "                next_value = next_value.squeeze(0)\n",
        "\n",
        "            # GAE-Lambda advantages\n",
        "            advs: List[torch.Tensor] = []\n",
        "            gae = torch.tensor(0.0, dtype=torch.float32, device=self.device)\n",
        "            prev_value = next_value\n",
        "            for t in reversed(range(len(rewards))):\n",
        "                delta = rewards[t] + self.gamma * prev_value * masks[t] - values[t]\n",
        "                gae = delta + self.gamma * self.gae_lambda * masks[t] * gae\n",
        "                advs.insert(0, gae)\n",
        "                prev_value = values[t]\n",
        "\n",
        "            returns = [advs[i] + values[i] for i in range(len(values))]\n",
        "\n",
        "            # Stack\n",
        "            returns_t = torch.stack(returns)\n",
        "            advs_t = torch.stack(advs)\n",
        "            values_t = torch.stack(values)\n",
        "            logps_t = torch.stack(logps)\n",
        "            entropies_t = torch.stack(entropies)\n",
        "\n",
        "            # Normalize advantages to reduce variance\n",
        "            if normalize_advantage:\n",
        "                advs_t = (advs_t - advs_t.mean()) / (advs_t.std(unbiased=False) + 1e-8)\n",
        "\n",
        "            # Anneal entropy coef from initial -> 0.001 over training\n",
        "            if entropy_anneal:\n",
        "                frac = 1.0 - (epoch / max(1, epochs))\n",
        "                ent_coef = 0.001 + frac * (self.entropy_coef_init - 0.001)\n",
        "            else:\n",
        "                ent_coef = self.entropy_coef_init\n",
        "\n",
        "            # Losses\n",
        "            policy_loss = - (logps_t * advs_t.detach()).mean()\n",
        "            value_loss = F.mse_loss(values_t, returns_t.detach())\n",
        "            entropy_loss = entropies_t.mean()\n",
        "            loss = policy_loss + self.value_coef * value_loss - ent_coef * entropy_loss\n",
        "\n",
        "            # Optimize\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, self.policy.parameters()), self.max_grad_norm)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Basic prints\n",
        "            if epoch % print_every == 0:\n",
        "                avg_return = returns_t.mean().item() if returns_t.numel() > 0 else 0.0\n",
        "                print(f\"[Epoch {epoch}] \"\n",
        "                      f\"loss={loss.item():.4f} policy_loss={policy_loss.item():.4f} \"\n",
        "                      f\"value_loss={value_loss.item():.4f} entropy={entropy_loss.item():.4f} \"\n",
        "                      f\"avg_return={avg_return:.3f} ent_coef={ent_coef:.4f} \"\n",
        "                      f\"episodes={episodes} ep_return_ma={self.ep_return_ma:.3f}\")\n",
        "\n",
        "            # Optional simple curriculum: if learning well, increase grid size\n",
        "            if self.curriculum and (epoch % 50 == 0) and (self.ep_return_ma > 0.6) and (self.env.size < 7):\n",
        "                self.env.size += 1\n",
        "                self.env.max_steps += 10\n",
        "                # reposition goal to new bottom-right\n",
        "                self.env.goal_pos = [self.env.size - 1, self.env.size - 1]\n",
        "                print(f\"Curriculum: increased grid to {self.env.size}x{self.env.size}\")\n",
        "\n",
        "# ===============================\n",
        "# Main\n",
        "# ===============================\n",
        "def main():\n",
        "    set_seed(123)\n",
        "\n",
        "    # Environment with shaping; tweak coefficients as needed\n",
        "    env = SimpleGridEnv(\n",
        "        size=5,\n",
        "        max_steps=50,\n",
        "        seed=123,\n",
        "        step_penalty=-0.01,\n",
        "        shaping_coef=0.1,\n",
        "        gamma=0.99,\n",
        "    )\n",
        "\n",
        "    obs_dim = env.observation_shape[0]\n",
        "    n_actions = env.action_space_n\n",
        "\n",
        "    # Reservoir QLSTM\n",
        "    qlstm_hidden = 32\n",
        "    qlstm = QLSTM(input_size=obs_dim, hidden_size=qlstm_hidden)\n",
        "    reservoir = ReservoirQLSTM(qlstm=qlstm, freeze=True, device=\"cpu\")\n",
        "    # Actor-Critic heads on top of reservoir features\n",
        "    reservoir.reset(batch_size=1)\n",
        "    policy = ActorCritic(input_dim=reservoir.feature_dim, n_actions=n_actions, hidden=128)\n",
        "\n",
        "    # Trainer\n",
        "    trainer = OnPolicyTrainer(\n",
        "        env=env,\n",
        "        reservoir=reservoir,\n",
        "        policy=policy,\n",
        "        device=\"cpu\",\n",
        "        gamma=0.99,\n",
        "        gae_lambda=0.95,\n",
        "        lr=1e-4,\n",
        "        entropy_coef=0.02,\n",
        "        value_coef=0.5,\n",
        "        max_grad_norm=0.5,\n",
        "        curriculum=False,  # set True to gradually increase grid size\n",
        "    )\n",
        "\n",
        "    t0 = time.time()\n",
        "    trainer.train(\n",
        "        epochs=400,            # increase for better performance\n",
        "        rollout_len=32,        # 32-64 works well here\n",
        "        print_every=20,\n",
        "        normalize_advantage=True,\n",
        "        entropy_anneal=True,\n",
        "    )\n",
        "    t1 = time.time()\n",
        "    print(f\"Training complete. Elapsed: {t1 - t0:.2f}s\")\n",
        "\n",
        "    # Save learned policy head (reservoir is frozen)\n",
        "    torch.save(policy.state_dict(), \"actor_critic_head.pth\")\n",
        "    print(\"Saved policy head to actor_critic_head.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wXAyvhpLAyZ",
        "outputId": "11838fe5-810e-4c20-e747-9147845098c7"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 20] loss=0.1063 policy_loss=-0.0016 value_loss=0.2686 entropy=1.3863 avg_return=0.210 ent_coef=0.0191 episodes=14 ep_return_ma=0.405\n",
            "[Epoch 40] loss=-0.0199 policy_loss=-0.0029 value_loss=0.0162 entropy=1.3863 avg_return=-0.049 ent_coef=0.0181 episodes=28 ep_return_ma=0.478\n",
            "[Epoch 60] loss=0.1721 policy_loss=-0.0028 value_loss=0.3974 entropy=1.3862 avg_return=0.509 ent_coef=0.0171 episodes=44 ep_return_ma=0.819\n",
            "[Epoch 80] loss=0.1441 policy_loss=-0.0057 value_loss=0.3445 entropy=1.3861 avg_return=0.245 ent_coef=0.0162 episodes=60 ep_return_ma=0.782\n",
            "[Epoch 100] loss=-0.0211 policy_loss=-0.0073 value_loss=0.0146 entropy=1.3860 avg_return=0.014 ent_coef=0.0152 episodes=75 ep_return_ma=0.673\n",
            "[Epoch 120] loss=-0.0074 policy_loss=-0.0070 value_loss=0.0388 entropy=1.3859 avg_return=0.060 ent_coef=0.0143 episodes=90 ep_return_ma=0.770\n",
            "[Epoch 140] loss=-0.0230 policy_loss=-0.0100 value_loss=0.0110 entropy=1.3856 avg_return=0.139 ent_coef=0.0134 episodes=105 ep_return_ma=0.728\n",
            "[Epoch 160] loss=-0.0265 policy_loss=-0.0185 value_loss=0.0184 entropy=1.3854 avg_return=0.171 ent_coef=0.0124 episodes=119 ep_return_ma=0.691\n",
            "[Epoch 180] loss=0.1651 policy_loss=-0.0133 value_loss=0.3886 entropy=1.3851 avg_return=0.694 ent_coef=0.0115 episodes=136 ep_return_ma=0.784\n",
            "[Epoch 200] loss=-0.0181 policy_loss=-0.0216 value_loss=0.0362 entropy=1.3845 avg_return=-0.001 ent_coef=0.0105 episodes=153 ep_return_ma=0.794\n",
            "[Epoch 220] loss=-0.0033 policy_loss=-0.0083 value_loss=0.0365 entropy=1.3839 avg_return=0.022 ent_coef=0.0095 episodes=168 ep_return_ma=0.990\n",
            "[Epoch 240] loss=-0.0268 policy_loss=-0.0262 value_loss=0.0226 entropy=1.3832 avg_return=0.090 ent_coef=0.0086 episodes=184 ep_return_ma=0.885\n",
            "[Epoch 260] loss=-0.0072 policy_loss=-0.0186 value_loss=0.0438 entropy=1.3823 avg_return=0.085 ent_coef=0.0076 episodes=201 ep_return_ma=0.953\n",
            "[Epoch 280] loss=0.0942 policy_loss=-0.0304 value_loss=0.2678 entropy=1.3813 avg_return=0.400 ent_coef=0.0067 episodes=217 ep_return_ma=0.991\n",
            "[Epoch 300] loss=-0.0529 policy_loss=-0.0494 value_loss=0.0089 entropy=1.3785 avg_return=0.228 ent_coef=0.0057 episodes=233 ep_return_ma=1.053\n",
            "[Epoch 320] loss=0.1262 policy_loss=-0.0454 value_loss=0.3565 entropy=1.3770 avg_return=0.697 ent_coef=0.0048 episodes=252 ep_return_ma=1.164\n",
            "[Epoch 340] loss=0.2004 policy_loss=-0.0510 value_loss=0.5134 entropy=1.3742 avg_return=0.959 ent_coef=0.0039 episodes=271 ep_return_ma=1.187\n",
            "[Epoch 360] loss=-0.0047 policy_loss=-0.0357 value_loss=0.0700 entropy=1.3712 avg_return=0.186 ent_coef=0.0029 episodes=290 ep_return_ma=1.198\n",
            "[Epoch 380] loss=0.0055 policy_loss=-0.0515 value_loss=0.1194 entropy=1.3664 avg_return=0.065 ent_coef=0.0020 episodes=307 ep_return_ma=1.049\n",
            "[Epoch 400] loss=0.0109 policy_loss=-0.0918 value_loss=0.2082 entropy=1.3623 avg_return=0.639 ent_coef=0.0010 episodes=326 ep_return_ma=1.148\n",
            "Training complete. Elapsed: 17.10s\n",
            "Saved policy head to actor_critic_head.pth\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\n",
        "from typing import Tuple, List, Optional\n",
        "import argparse\n",
        "import math\n",
        "import time\n",
        "import copy\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import sys\n",
        "\n",
        "try:\n",
        "    import pennylane as qml\n",
        "except Exception:\n",
        "    qml = None\n",
        "    print(\"WARNING: pennylane not available. Set up pennylane if you want quantum circuits.\")\n",
        "\n",
        "# -------------------------\n",
        "# Hyperparameters (paper-inspired)\n",
        "# -------------------------\n",
        "DEFAULT_LR = 1e-4\n",
        "BETA1 = 0.92\n",
        "BETA2 = 0.999\n",
        "GAMMA = 0.9                 # per paper\n",
        "GAE_LAMBDA = 0.95\n",
        "ENTROPY_INIT = 0.02\n",
        "VALUE_COEF = 0.5\n",
        "MAX_GRAD_NORM = 0.5\n",
        "\n",
        "# -------------------------\n",
        "# Utilities\n",
        "# -------------------------\n",
        "def set_seed(seed: int = 1234):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "# -------------------------\n",
        "# Small MiniGrid-like environment (no gym)\n",
        "# Partial observation (local view) to make memory beneficial.\n",
        "# -------------------------\n",
        "class MiniGridLite:\n",
        "    def __init__(self, size: int = 5, max_steps: int = 4 * 5, view_size: int = 3,\n",
        "                 step_penalty: float = -0.01, shaping_coef: float = 0.1, gamma: float = GAMMA,\n",
        "                 random_start: bool = False, seed: Optional[int] = None):\n",
        "        assert view_size % 2 == 1\n",
        "        self.size = int(size)\n",
        "        self.max_steps = int(max_steps)\n",
        "        self.view_size = int(view_size)\n",
        "        self.step_penalty = float(step_penalty)\n",
        "        self.shaping_coef = float(shaping_coef)\n",
        "        self.gamma = float(gamma)\n",
        "        self.random_start = bool(random_start)\n",
        "        self.rng = np.random.RandomState(seed)\n",
        "        self.reset()\n",
        "\n",
        "    @property\n",
        "    def observation_shape(self) -> Tuple[int]:\n",
        "        view_elems = self.view_size * self.view_size * 2\n",
        "        return (view_elems + 2,)\n",
        "\n",
        "    @property\n",
        "    def action_space_n(self) -> int:\n",
        "        return 4\n",
        "\n",
        "    def _phi(self, pos: List[int]) -> float:\n",
        "        return - (abs(pos[0] - self.goal_pos[0]) + abs(pos[1] - self.goal_pos[1]))\n",
        "\n",
        "    def reset(self) -> np.ndarray:\n",
        "        if self.random_start:\n",
        "            self.agent_pos = [int(self.rng.randint(0, self.size)), int(self.rng.randint(0, self.size))]\n",
        "        else:\n",
        "            self.agent_pos = [0, 0]\n",
        "        self.goal_pos = [self.size - 1, self.size - 1]\n",
        "        self.steps = 0\n",
        "        self._last_phi = self._phi(self.agent_pos)\n",
        "        return self._render_obs()\n",
        "\n",
        "    def step(self, action: int):\n",
        "        prev_phi = self._last_phi\n",
        "        if action == 0:\n",
        "            self.agent_pos[0] = max(0, self.agent_pos[0] - 1)\n",
        "        elif action == 1:\n",
        "            self.agent_pos[1] = min(self.size - 1, self.agent_pos[1] + 1)\n",
        "        elif action == 2:\n",
        "            self.agent_pos[0] = min(self.size - 1, self.agent_pos[0] + 1)\n",
        "        elif action == 3:\n",
        "            self.agent_pos[1] = max(0, self.agent_pos[1] - 1)\n",
        "        else:\n",
        "            raise ValueError(\"action must be 0..3\")\n",
        "\n",
        "        self.steps += 1\n",
        "        reached = (self.agent_pos == self.goal_pos)\n",
        "        done = reached or (self.steps >= self.max_steps)\n",
        "        base_reward = 1.0 if reached else 0.0\n",
        "        new_phi = self._phi(self.agent_pos)\n",
        "        shaping = self.shaping_coef * (self.gamma * new_phi - prev_phi)\n",
        "        r = base_reward + shaping + (0.0 if reached else self.step_penalty)\n",
        "        self._last_phi = new_phi\n",
        "        obs = self._render_obs()\n",
        "        return obs, float(r), bool(done), {}\n",
        "\n",
        "    def _render_obs(self) -> np.ndarray:\n",
        "        half = self.view_size // 2\n",
        "        ax, ay = self.agent_pos\n",
        "        view = np.zeros((self.view_size, self.view_size, 2), dtype=np.float32)\n",
        "        for dx in range(-half, half + 1):\n",
        "            for dy in range(-half, half + 1):\n",
        "                x = ax + dx; y = ay + dy\n",
        "                xi = dx + half; yi = dy + half\n",
        "                if 0 <= x < self.size and 0 <= y < self.size:\n",
        "                    view[xi, yi, 0] = 1.0\n",
        "                    if [x, y] == self.goal_pos:\n",
        "                        view[xi, yi, 1] = 1.0\n",
        "                        view[xi, yi, 0] = 0.0\n",
        "                else:\n",
        "                    view[xi, yi, 0] = 0.0\n",
        "                    view[xi, yi, 1] = 0.0\n",
        "        relx = (self.goal_pos[0] - self.agent_pos[0]) / float(max(1, self.size - 1))\n",
        "        rely = (self.goal_pos[1] - self.agent_pos[1]) / float(max(1, self.size - 1))\n",
        "        extra = np.array([relx, rely], dtype=np.float32)\n",
        "        obs = np.concatenate([view.ravel(), extra]).astype(np.float32)\n",
        "        return obs\n",
        "\n",
        "# -------------------------\n",
        "# PennyLane QLSTM cell (VQC)\n",
        "# - encoding: H + RY(arctan(x)) + RZ(arctan(x^2))\n",
        "# - chain CNOT entangler\n",
        "# - Rot(a,b,c) variational per wire per layer\n",
        "# - measure Z per wire -> classical features -> gate readout\n",
        "# -------------------------\n",
        "class PennyQLSTMCell(nn.Module):\n",
        "    def __init__(self, in_dim: int, hidden_dim: int, n_qubits: int = 4, n_layers: int = 1,\n",
        "                 device_name: str = \"default.qubit\", freeze: bool = True):\n",
        "        super().__init__()\n",
        "        if qml is None:\n",
        "            raise RuntimeError(\"PennyLane required for PennyQLSTMCell. Install pennylane.\")\n",
        "\n",
        "        self.in_dim = int(in_dim)\n",
        "        self.hidden_dim = int(hidden_dim)\n",
        "        self.n_qubits = int(n_qubits)\n",
        "        self.n_layers = int(n_layers)\n",
        "        self.device_name = device_name\n",
        "        self.freeze = bool(freeze)\n",
        "\n",
        "        # classical encoder and readout\n",
        "        self.encode_lin = nn.Linear(self.in_dim + self.hidden_dim, self.n_qubits)\n",
        "        nn.init.xavier_uniform_(self.encode_lin.weight)\n",
        "        nn.init.constant_(self.encode_lin.bias, 0.0)\n",
        "\n",
        "        self.gate_lin = nn.Linear(self.n_qubits, 4 * self.hidden_dim)\n",
        "        nn.init.xavier_uniform_(self.gate_lin.weight)\n",
        "        nn.init.constant_(self.gate_lin.bias, 0.0)\n",
        "\n",
        "        # quantum params (3 per qubit per layer)\n",
        "        self.total_qparams = self.n_layers * self.n_qubits * 3\n",
        "        init_q = 0.05 * torch.randn(self.total_qparams)\n",
        "        self.q_params = nn.Parameter(init_q, requires_grad=(not self.freeze))\n",
        "\n",
        "        # lazy qnode\n",
        "        self._qnode = None\n",
        "\n",
        "    def build_qnode(self):\n",
        "        # create process-local device / qnode (here single-process)\n",
        "        dev = qml.device(self.device_name, wires=self.n_qubits)\n",
        "\n",
        "        @qml.qnode(dev, interface=\"torch\", diff_method=\"parameter-shift\")\n",
        "        def qnode(x_enc, qparams):\n",
        "            # encoding\n",
        "            for w in range(self.n_qubits):\n",
        "                qml.H(wires=w)\n",
        "                qml.RY(torch.atan(x_enc[w]), wires=w)\n",
        "                qml.RZ(torch.atan(x_enc[w] ** 2), wires=w)\n",
        "            idx = 0\n",
        "            for _ in range(self.n_layers):\n",
        "                for w in range(self.n_qubits - 1):\n",
        "                    qml.CNOT(wires=[w, w + 1])\n",
        "                for w in range(self.n_qubits):\n",
        "                    a = qparams[idx]; b = qparams[idx + 1]; c = qparams[idx + 2]\n",
        "                    qml.Rot(a, b, c, wires=w)\n",
        "                    idx += 3\n",
        "            return [qml.expval(qml.PauliZ(w)) for w in range(self.n_qubits)]\n",
        "\n",
        "        self._qnode = qnode\n",
        "\n",
        "    def forward(self, obs_vec: torch.Tensor, h_prev: torch.Tensor, c_prev: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        if self._qnode is None:\n",
        "            self.build_qnode()\n",
        "\n",
        "        combined = torch.cat([obs_vec, h_prev], dim=0)\n",
        "        x_enc = self.encode_lin(combined)  # shape (n_qubits,)\n",
        "\n",
        "        # execute qnode: prefer torch.Tensor result\n",
        "        q_out = self._qnode(x_enc, self.q_params)\n",
        "        q_feats = torch.as_tensor(q_out, dtype=torch.float32)\n",
        "\n",
        "        # safe conversion and device handling:\n",
        "        #if isinstance(q_out, torch.Tensor):\n",
        "        #    q_feats = q_out\n",
        "       # else:\n",
        "            # if q_out is list/np, convert to CPU float tensor\n",
        "       #     q_feats = torch.tensor(q_out, dtype=torch.float32, device='cpu')\n",
        "\n",
        "        # if reservoir frozen, detach quantum outputs immediately to avoid autograd graphs linking into quantum side\n",
        "        if self.freeze:\n",
        "            q_feats = q_feats.detach()\n",
        "\n",
        "        # move q_feats to same device as obs_vec (likely CPU)\n",
        "        q_feats = q_feats.to(obs_vec.device)\n",
        "\n",
        "        gates = self.gate_lin(q_feats)  # (4 * hidden_dim,)\n",
        "        i, f, o, g = gates.chunk(4, dim=-1)\n",
        "        i = torch.sigmoid(i); f = torch.sigmoid(f); o = torch.sigmoid(o); g = torch.tanh(g)\n",
        "        c_next = f * c_prev + i * g\n",
        "        h_next = o * torch.tanh(c_next)\n",
        "        return h_next, c_next\n",
        "\n",
        "# -------------------------\n",
        "# ActorCritic heads (classical)\n",
        "# -------------------------\n",
        "class ActorCritic(nn.Module):\n",
        "    def __init__(self, input_dim: int, n_actions: int, hidden: int = 128):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden)\n",
        "        self.pi = nn.Linear(hidden, n_actions)\n",
        "        self.v = nn.Linear(hidden, 1)\n",
        "        nn.init.orthogonal_(self.fc1.weight, gain=math.sqrt(2))\n",
        "        nn.init.constant_(self.fc1.bias, 0.0)\n",
        "        nn.init.orthogonal_(self.pi.weight, gain=0.01)\n",
        "        nn.init.constant_(self.pi.bias, 0.0)\n",
        "        nn.init.orthogonal_(self.v.weight, gain=1.0)\n",
        "        nn.init.constant_(self.v.bias, 0.0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        logits = self.pi(x)\n",
        "        value = self.v(x).squeeze(-1)\n",
        "        return logits, value\n",
        "\n",
        "# -------------------------\n",
        "# Hybrid model: classical preproc -> QLSTM cell -> actor/critic (classical)\n",
        "# -------------------------\n",
        "class HybridModel(nn.Module):\n",
        "    def __init__(self, obs_dim: int, n_actions: int, q_hidden: int = 32, q_n_qubits: int = 4, q_n_layers: int = 1,\n",
        "                 preproc_hidden: int = 64, head_hidden: int = 128, freeze_quantum: bool = True, q_device: str = \"default.qubit\"):\n",
        "        super().__init__()\n",
        "        self.preproc = nn.Sequential(nn.Linear(obs_dim, preproc_hidden), nn.ReLU())\n",
        "        self.qlstm_cell = PennyQLSTMCell(in_dim=preproc_hidden, hidden_dim=q_hidden, n_qubits=q_n_qubits, n_layers=q_n_layers, device_name=q_device, freeze=freeze_quantum)\n",
        "        self.actor_head = nn.Sequential(nn.Linear(q_hidden, head_hidden), nn.ReLU(), nn.Linear(head_hidden, n_actions))\n",
        "        self.critic_head = nn.Sequential(nn.Linear(q_hidden, head_hidden), nn.ReLU(), nn.Linear(head_hidden, 1))\n",
        "\n",
        "        # init classical linears\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Linear):\n",
        "                nn.init.orthogonal_(m.weight, gain=math.sqrt(2))\n",
        "                nn.init.constant_(m.bias, 0.0)\n",
        "\n",
        "    def init_hidden(self, device: Optional[torch.device] = None):\n",
        "        device = device or torch.device(\"cpu\")\n",
        "        return torch.zeros(self.qlstm_cell.hidden_dim, device=device), torch.zeros(self.qlstm_cell.hidden_dim, device=device)\n",
        "\n",
        "    def forward_step(self, obs_np: np.ndarray, h_prev: torch.Tensor, c_prev: torch.Tensor):\n",
        "        obs = torch.tensor(obs_np, dtype=torch.float32)\n",
        "        x = self.preproc(obs)\n",
        "        h_next, c_next = self.qlstm_cell.forward(x, h_prev, c_prev)\n",
        "        logits = self.actor_head(h_next)\n",
        "        value = self.critic_head(h_next).squeeze(-1)\n",
        "        return logits, value, (h_next, c_next)\n",
        "\n",
        "# -------------------------\n",
        "# On-policy trainer (single-process)\n",
        "# - collects rollouts of fixed length, computes GAE, advantage normalization,\n",
        "#   entropy annealing, and performs single optimizer.step per update.\n",
        "# -------------------------\n",
        "class OnPolicyTrainer:\n",
        "    def __init__(self, env: MiniGridLite, model: HybridModel, device: str = \"cpu\",\n",
        "                 lr: float = DEFAULT_LR, gamma: float = GAMMA, gae_lambda: float = GAE_LAMBDA,\n",
        "                 entropy_coef_init: float = ENTROPY_INIT, value_coef: float = VALUE_COEF,\n",
        "                 max_grad_norm: float = MAX_GRAD_NORM):\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "        self.device = torch.device(device)\n",
        "        self.gamma = float(gamma)\n",
        "        self.gae_lambda = float(gae_lambda)\n",
        "        self.entropy_coef_init = float(entropy_coef_init)\n",
        "        self.value_coef = float(value_coef)\n",
        "        self.max_grad_norm = float(max_grad_norm)\n",
        "\n",
        "        # optimizer: include quantum params only if they are trainable\n",
        "        params = [p for p in self.model.parameters() if p.requires_grad]\n",
        "        if len(params) == 0:\n",
        "            raise RuntimeError(\"No trainable params found (maybe quantum reservoir frozen and no classical params?).\")\n",
        "        self.optimizer = optim.Adam(params, lr=lr, betas=(BETA1, BETA2))\n",
        "\n",
        "        self.ep_return_ma = 0.0\n",
        "        self.ma_tau = 0.95\n",
        "\n",
        "    def train(self, epochs: int = 400, rollout_len: int = 32, print_every: int = 10,\n",
        "              normalize_advantage: bool = True, entropy_anneal: bool = True):\n",
        "        obs = self.env.reset()\n",
        "        h, c = self.model.init_hidden()\n",
        "        episode_return = 0.0\n",
        "        episodes = 0\n",
        "\n",
        "        for epoch in range(1, epochs + 1):\n",
        "            rewards: List[torch.Tensor] = []\n",
        "            values: List[torch.Tensor] = []\n",
        "            logps: List[torch.Tensor] = []\n",
        "            entropies: List[torch.Tensor] = []\n",
        "            masks: List[torch.Tensor] = []\n",
        "\n",
        "            # collect rollout\n",
        "            for t in range(rollout_len):\n",
        "                logits, value, (h, c) = self.model.forward_step(obs, h, c)\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                dist = torch.distributions.Categorical(probs=probs)\n",
        "                action = dist.sample()\n",
        "                logp = dist.log_prob(action)\n",
        "                entropy = dist.entropy().mean()\n",
        "\n",
        "                next_obs, reward, done, _ = self.env.step(int(action.item()))\n",
        "                episode_return += reward\n",
        "\n",
        "                rewards.append(torch.tensor(reward, dtype=torch.float32))\n",
        "                values.append(value)\n",
        "                logps.append(logp)\n",
        "                entropies.append(entropy)\n",
        "                masks.append(torch.tensor(0.0 if done else 1.0, dtype=torch.float32))\n",
        "\n",
        "                obs = next_obs\n",
        "                if done:\n",
        "                    obs = self.env.reset()\n",
        "                    h, c = self.model.init_hidden()\n",
        "                    episodes += 1\n",
        "                    self.ep_return_ma = self.ma_tau * self.ep_return_ma + (1 - self.ma_tau) * episode_return\n",
        "                    episode_return = 0.0\n",
        "\n",
        "            # bootstrap from last obs without advancing QLSTM (we used forward_step so state already advanced)\n",
        "            # We will compute next value using current hidden (h,c) but not advance quantum state\n",
        "            with torch.no_grad():\n",
        "                logits, next_value, _ = self.model.forward_step(obs, h, c)\n",
        "                next_value = next_value\n",
        "\n",
        "            # compute GAE\n",
        "            advs: List[torch.Tensor] = []\n",
        "            gae = 0.0\n",
        "            prev_value = next_value\n",
        "            for i in reversed(range(len(rewards))):\n",
        "                delta = rewards[i].to(self.device) + self.gamma * prev_value * masks[i].to(self.device) - values[i].to(self.device)\n",
        "                gae = delta + self.gamma * self.gae_lambda * masks[i].to(self.device) * gae\n",
        "                advs.insert(0, gae)\n",
        "                prev_value = values[i].to(self.device)\n",
        "\n",
        "            returns = [advs[i] + values[i].to(self.device) for i in range(len(values))]\n",
        "\n",
        "            # stack for loss\n",
        "            returns_t = torch.stack(returns)\n",
        "            advs_t = torch.stack(advs)\n",
        "            values_t = torch.stack([v.to(self.device) for v in values])\n",
        "            logps_t = torch.stack([lp.to(self.device) for lp in logps])\n",
        "            entropies_t = torch.stack([e.to(self.device) for e in entropies])\n",
        "\n",
        "            if normalize_advantage:\n",
        "                advs_t = (advs_t - advs_t.mean()) / (advs_t.std(unbiased=False) + 1e-8)\n",
        "\n",
        "            # anneal entropy\n",
        "            if entropy_anneal:\n",
        "                frac = 1.0 - (epoch / max(1, epochs))\n",
        "                ent_coef = 0.001 + frac * (self.entropy_coef_init - 0.001)\n",
        "            else:\n",
        "                ent_coef = self.entropy_coef_init\n",
        "\n",
        "            policy_loss = - (logps_t * advs_t.detach()).mean()\n",
        "            value_loss = F.mse_loss(values_t, returns_t.detach())\n",
        "            entropy_loss = entropies_t.mean()\n",
        "            loss = policy_loss + self.value_coef * value_loss - ent_coef * entropy_loss\n",
        "\n",
        "            # optimize\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward(retain_graph=True)\n",
        "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)\n",
        "            self.optimizer.step()\n",
        "\n",
        "            if epoch % print_every == 0:\n",
        "                avg_return = returns_t.mean().item() if returns_t.numel() > 0 else 0.0\n",
        "                print(f\"[Epoch {epoch}] loss={loss.item():.4f} policy_loss={policy_loss.item():.4f} value_loss={value_loss.item():.4f} entropy={entropy_loss.item():.4f} avg_return={avg_return:.3f} ep_return_ma={self.ep_return_ma:.3f} ent_coef={ent_coef:.4f}\")\n",
        "\n",
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "def main():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--epochs\", type=int, default=400)\n",
        "    parser.add_argument(\"--rollout_len\", type=int, default=32)\n",
        "    parser.add_argument(\"--print_every\", type=int, default=10)\n",
        "    parser.add_argument(\"--grid_size\", type=int, default=5)\n",
        "    parser.add_argument(\"--view_size\", type=int, default=3)\n",
        "    parser.add_argument(\"--q_n_qubits\", type=int, default=4)\n",
        "    parser.add_argument(\"--q_n_layers\", type=int, default=2)\n",
        "    parser.add_argument(\"--q_hidden\", type=int, default=32)\n",
        "    parser.add_argument(\"--preproc_hidden\", type=int, default=64)\n",
        "    parser.add_argument(\"--head_hidden\", type=int, default=128)\n",
        "    parser.add_argument(\"--lr\", type=float, default=DEFAULT_LR)\n",
        "    parser.add_argument(\"--train-quantum\", action=\"store_true\", help=\"Include quantum params in optimization (slow).\")\n",
        "    args, unknown = parser.parse_known_args(sys.argv[1:])\n",
        "\n",
        "    set_seed(1234)\n",
        "\n",
        "    # env\n",
        "    env = MiniGridLite(size=args.grid_size, max_steps=4 * args.grid_size, view_size=args.view_size)\n",
        "\n",
        "    obs_dim = env.observation_shape[0]\n",
        "    n_actions = env.action_space_n\n",
        "\n",
        "    model = HybridModel(obs_dim=obs_dim, n_actions=n_actions,\n",
        "                        q_hidden=args.q_hidden, q_n_qubits=args.q_n_qubits, q_n_layers=args.q_n_layers,\n",
        "                        preproc_hidden=args.preproc_hidden, head_hidden=args.head_hidden,\n",
        "                        freeze_quantum=(not args.train_quantum),\n",
        "                        q_device=(\"lightning.qubit\" if qml is not None else \"default.qubit\"))\n",
        "\n",
        "    trainer = OnPolicyTrainer(env=env, model=model, device=\"cpu\", lr=args.lr,\n",
        "                              gamma=GAMMA, gae_lambda=GAE_LAMBDA,\n",
        "                              entropy_coef_init=ENTROPY_INIT, value_coef=VALUE_COEF,\n",
        "                              max_grad_norm=MAX_GRAD_NORM)\n",
        "\n",
        "    t0 = time.time()\n",
        "    trainer.train(epochs=args.epochs, rollout_len=args.rollout_len, print_every=args.print_every)\n",
        "    t1 = time.time()\n",
        "    print(f\"Training done. Elapsed {t1 - t0:.2f}s\")\n",
        "\n",
        "    torch.save(model.state_dict(), \"qlstm_sync_model.pth\")\n",
        "    print(\"Saved model to qlstm_sync_model.pth\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Mxia8hTPFNn",
        "outputId": "a09c6860-cb41-4e1e-b9ae-24458a0a0003"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 10] loss=0.0198 policy_loss=0.0014 value_loss=0.0910 entropy=1.3863 avg_return=0.235 ep_return_ma=0.732 ent_coef=0.0195\n",
            "[Epoch 20] loss=0.0134 policy_loss=0.0015 value_loss=0.0766 entropy=1.3863 avg_return=0.204 ep_return_ma=1.088 ent_coef=0.0191\n",
            "[Epoch 30] loss=0.0151 policy_loss=-0.0017 value_loss=0.0851 entropy=1.3862 avg_return=0.230 ep_return_ma=1.191 ent_coef=0.0186\n",
            "[Epoch 40] loss=0.0257 policy_loss=-0.0036 value_loss=0.1086 entropy=1.3862 avg_return=0.343 ep_return_ma=1.229 ent_coef=0.0181\n",
            "[Epoch 50] loss=0.0115 policy_loss=-0.0036 value_loss=0.0791 entropy=1.3862 avg_return=0.275 ep_return_ma=1.321 ent_coef=0.0176\n",
            "[Epoch 60] loss=-0.0009 policy_loss=-0.0066 value_loss=0.0589 entropy=1.3861 avg_return=0.214 ep_return_ma=1.374 ent_coef=0.0171\n",
            "[Epoch 70] loss=0.0038 policy_loss=-0.0090 value_loss=0.0719 entropy=1.3860 avg_return=0.266 ep_return_ma=1.328 ent_coef=0.0167\n",
            "[Epoch 80] loss=0.0982 policy_loss=-0.0126 value_loss=0.2665 entropy=1.3860 avg_return=0.525 ep_return_ma=1.363 ent_coef=0.0162\n",
            "[Epoch 90] loss=0.0143 policy_loss=-0.0050 value_loss=0.0822 entropy=1.3858 avg_return=0.306 ep_return_ma=1.293 ent_coef=0.0157\n",
            "[Epoch 100] loss=0.1297 policy_loss=-0.0113 value_loss=0.3242 entropy=1.3857 avg_return=0.547 ep_return_ma=1.379 ent_coef=0.0152\n",
            "[Epoch 110] loss=-0.0012 policy_loss=-0.0097 value_loss=0.0578 entropy=1.3854 avg_return=0.241 ep_return_ma=1.353 ent_coef=0.0148\n",
            "[Epoch 120] loss=0.0918 policy_loss=-0.0214 value_loss=0.2660 entropy=1.3852 avg_return=0.516 ep_return_ma=1.498 ent_coef=0.0143\n",
            "[Epoch 130] loss=0.0033 policy_loss=-0.0057 value_loss=0.0564 entropy=1.3850 avg_return=0.306 ep_return_ma=1.432 ent_coef=0.0138\n",
            "[Epoch 140] loss=0.0026 policy_loss=-0.0144 value_loss=0.0710 entropy=1.3846 avg_return=0.399 ep_return_ma=1.329 ent_coef=0.0134\n",
            "[Epoch 150] loss=0.0047 policy_loss=-0.0103 value_loss=0.0657 entropy=1.3846 avg_return=0.327 ep_return_ma=1.299 ent_coef=0.0129\n",
            "[Epoch 160] loss=-0.0288 policy_loss=-0.0310 value_loss=0.0386 entropy=1.3830 avg_return=0.198 ep_return_ma=1.353 ent_coef=0.0124\n",
            "[Epoch 170] loss=0.0726 policy_loss=-0.0139 value_loss=0.2061 entropy=1.3840 avg_return=0.501 ep_return_ma=1.436 ent_coef=0.0119\n",
            "[Epoch 180] loss=-0.0120 policy_loss=-0.0176 value_loss=0.0429 entropy=1.3830 avg_return=0.313 ep_return_ma=1.427 ent_coef=0.0115\n",
            "[Epoch 190] loss=0.0369 policy_loss=0.0150 value_loss=0.0741 entropy=1.3833 avg_return=0.390 ep_return_ma=1.345 ent_coef=0.0110\n",
            "[Epoch 200] loss=0.0525 policy_loss=-0.0466 value_loss=0.2271 entropy=1.3825 avg_return=0.612 ep_return_ma=1.438 ent_coef=0.0105\n",
            "[Epoch 210] loss=-0.0385 policy_loss=-0.0415 value_loss=0.0339 entropy=1.3801 avg_return=0.289 ep_return_ma=1.383 ent_coef=0.0100\n",
            "[Epoch 220] loss=0.0406 policy_loss=-0.0560 value_loss=0.2197 entropy=1.3808 avg_return=0.669 ep_return_ma=1.472 ent_coef=0.0095\n",
            "[Epoch 230] loss=-0.0204 policy_loss=-0.0413 value_loss=0.0668 entropy=1.3791 avg_return=0.276 ep_return_ma=1.389 ent_coef=0.0091\n",
            "[Epoch 240] loss=-0.0036 policy_loss=-0.0720 value_loss=0.1605 entropy=1.3805 avg_return=0.461 ep_return_ma=1.354 ent_coef=0.0086\n",
            "[Epoch 250] loss=0.0242 policy_loss=0.0047 value_loss=0.0613 entropy=1.3777 avg_return=0.379 ep_return_ma=1.424 ent_coef=0.0081\n",
            "[Epoch 260] loss=0.0162 policy_loss=0.0108 value_loss=0.0319 entropy=1.3773 avg_return=0.318 ep_return_ma=1.422 ent_coef=0.0076\n",
            "[Epoch 270] loss=-0.0728 policy_loss=-0.0820 value_loss=0.0381 entropy=1.3752 avg_return=0.372 ep_return_ma=1.365 ent_coef=0.0072\n",
            "[Epoch 280] loss=-0.0656 policy_loss=-0.0842 value_loss=0.0555 entropy=1.3765 avg_return=0.393 ep_return_ma=1.350 ent_coef=0.0067\n",
            "[Epoch 290] loss=0.0533 policy_loss=-0.0673 value_loss=0.2583 entropy=1.3742 avg_return=0.779 ep_return_ma=1.576 ent_coef=0.0062\n",
            "[Epoch 300] loss=-0.0248 policy_loss=-0.0530 value_loss=0.0721 entropy=1.3679 avg_return=0.363 ep_return_ma=1.500 ent_coef=0.0057\n",
            "[Epoch 310] loss=0.0147 policy_loss=-0.0552 value_loss=0.1542 entropy=1.3677 avg_return=0.524 ep_return_ma=1.493 ent_coef=0.0053\n",
            "[Epoch 320] loss=-0.0267 policy_loss=-0.0391 value_loss=0.0379 entropy=1.3672 avg_return=0.448 ep_return_ma=1.468 ent_coef=0.0048\n",
            "[Epoch 330] loss=0.0207 policy_loss=0.0030 value_loss=0.0473 entropy=1.3615 avg_return=0.435 ep_return_ma=1.396 ent_coef=0.0043\n",
            "[Epoch 340] loss=-0.0219 policy_loss=-0.0458 value_loss=0.0582 entropy=1.3612 avg_return=0.291 ep_return_ma=1.458 ent_coef=0.0039\n",
            "[Epoch 350] loss=-0.0362 policy_loss=-0.0557 value_loss=0.0482 entropy=1.3629 avg_return=0.447 ep_return_ma=1.474 ent_coef=0.0034\n",
            "[Epoch 360] loss=-0.0316 policy_loss=-0.1001 value_loss=0.1448 entropy=1.3557 avg_return=0.368 ep_return_ma=1.450 ent_coef=0.0029\n",
            "[Epoch 370] loss=-0.0255 policy_loss=-0.0649 value_loss=0.0853 entropy=1.3467 avg_return=0.597 ep_return_ma=1.482 ent_coef=0.0024\n",
            "[Epoch 380] loss=-0.0518 policy_loss=-0.1197 value_loss=0.1410 entropy=1.3531 avg_return=0.585 ep_return_ma=1.467 ent_coef=0.0020\n",
            "[Epoch 390] loss=-0.0418 policy_loss=-0.0555 value_loss=0.0312 entropy=1.3536 avg_return=0.345 ep_return_ma=1.411 ent_coef=0.0015\n",
            "[Epoch 400] loss=-0.0125 policy_loss=-0.0725 value_loss=0.1226 entropy=1.3334 avg_return=0.488 ep_return_ma=1.569 ent_coef=0.0010\n",
            "Training done. Elapsed 76.30s\n",
            "Saved model to qlstm_sync_model.pth\n"
          ]
        }
      ]
    }
  ]
}